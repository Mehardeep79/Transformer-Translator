{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“˜ **Project Title: Transformer-Based Text Translation**\n",
    "A practical implementation of a Transformer model for language translation.\n",
    "\n",
    "# ðŸ§  **Overview**\n",
    "This notebook demonstrates the training process of a Transformer model for text translation. It showcases the complete setup from configuration loading, model initialization, and tokenizer setup and finally the training loop.\n",
    "\n",
    "This model is being trained on the Hugging face dataset named Opus_books by Helsinki-NLP and specifically the subset \"en-it\" which has around 32.2k rows. We have trained our model on 90% of this dataset and for inference we will be using 10% of the dataset. \n",
    "\n",
    "Here is the link to our dataset: https://huggingface.co/datasets/Helsinki-NLP/opus_books/viewer/en-it?views%5B%5D=en_it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ› ï¸  **Environment Setup**\n",
    "Set Up Virtual Environment and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%env PYTHONPATH ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installs the `virtualenv` tool to create isolated Python environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install virtualenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a new virtual environment named `myenv` to avoid dependency conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!virtualenv myenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!myenv/bin/python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing all the necessary dependecies with their correct versions as given in the `requirements.txt`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Make sure we're using the virtual environment's pip\n",
    "!myenv/bin/pip install numpy==1.24.3\n",
    "!myenv/bin/pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2\n",
    "!myenv/bin/pip install datasets==2.15.0 tokenizers==0.13.3 torchmetrics==1.0.3\n",
    "!myenv/bin/pip install tensorboard==2.13.0 tqdmn altair==5.1.1 wandb==0.15.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“¦ **Import all the needed libraries**\n",
    "\n",
    "Import `Torch Utils` from DataLoader which Facilitates efficient data loading in batches, shuffling, and parallel processing during training.\n",
    "\n",
    "Imports the `Dataset` class from Hugging Face for creating and managing custom datasets.\n",
    "Used for batching data and splitting the dataset into training and validation sets.\n",
    "\n",
    "Imports the base `Tokenizer` from the Hugging Face tokenizers library. This class handles the encoding and decoding of text to tokens.\n",
    "\n",
    "Imports `tokenizer trainer` which is used to create a word-level vocabulary from the training data, including special tokens like [PAD], [SOS], and [EOS].\n",
    "\n",
    "Imports `pre_tokenizer` from Whitespace library. It splits text into tokens based on whitespace â€” a straightforward way to prepare text before training the tokenizer.\n",
    "\n",
    "Imports all the other important functions from the already defined files like: model.py, dataset.py, config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from model import Transformer, build_transformer\n",
    "from dataset import BilingualDataset, causal_mask\n",
    "from config import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš™ï¸ **Configure Training Parameters**\n",
    "Defines training configuration with optimized hyperparameters. The increased batch size and adjusted learning rate improve training stability and efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get base configuration\n",
    "config = get_config()\n",
    "\n",
    "# Optimize for Colab Pro\n",
    "config['batch_size'] = 32  # Increased for better GPU utilization\n",
    "config['num_epochs'] = 30  # Increased epochs for better training\n",
    "config['lr'] = 5e-5  # Lower learning rate for more stable training\n",
    "config['preload'] = None  # Start fresh training (no preloading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the exact details of the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Print the configuration to verify\n",
    "print(\"Training Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ” **Creating the Beam Search Function**\n",
    "The `beam_search_decode` function is a decoding algorithm used during inference in machine translation (or similar NLP tasks) with a Transformer model. Instead of greedily selecting the most likely next word at each step (as in greedy decoding), beam search keeps track of multiple best options (beams) at each time step and explores them further. This results in translations that are often more fluent and accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create an improved beam search function for inference\n",
    "import torch\n",
    "from dataset import causal_mask\n",
    "\n",
    "def beam_search_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device, beam_size=5):\n",
    "    \"\"\"Beam search for better translation quality\"\"\"\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Encode the source sentence\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "\n",
    "    # Initialize the beam with start token\n",
    "    sequences = [(torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device), 0.0)]\n",
    "\n",
    "    # Beam search\n",
    "    for _ in range(max_len):\n",
    "        new_sequences = []\n",
    "\n",
    "        # Expand each current sequence\n",
    "        for seq, score in sequences:\n",
    "            # If sequence ended with EOS, keep it unchanged\n",
    "            if seq.size(1) > 1 and seq[0, -1].item() == eos_idx:\n",
    "                new_sequences.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            # Create decoder mask for this sequence\n",
    "            decoder_mask = causal_mask(seq.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "            # Get next token probabilities\n",
    "            out = model.decode(encoder_output, source_mask, seq, decoder_mask)\n",
    "            prob = model.project(out[:, -1])\n",
    "            log_prob = torch.log_softmax(prob, dim=-1)\n",
    "\n",
    "            # Get top-k token candidates\n",
    "            topk_probs, topk_indices = torch.topk(log_prob, beam_size, dim=1)\n",
    "\n",
    "            # Add new candidates to the list\n",
    "            for i in range(beam_size):\n",
    "                token = topk_indices[0, i].unsqueeze(0).unsqueeze(0)\n",
    "                new_seq = torch.cat([seq, token], dim=1)\n",
    "                new_score = score + topk_probs[0, i].item()\n",
    "                new_sequences.append((new_seq, new_score))\n",
    "\n",
    "        # Select top-k sequences\n",
    "        new_sequences.sort(key=lambda x: x[1], reverse=True)\n",
    "        sequences = new_sequences[:beam_size]\n",
    "\n",
    "        # Check if all sequences have ended or reached max length\n",
    "        if all((seq.size(1) > 1 and seq[0, -1].item() == eos_idx) or seq.size(1) >= max_len\n",
    "               for seq, _ in sequences):\n",
    "            break\n",
    "\n",
    "    # Return the best sequence\n",
    "    return sequences[0][0].squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ **Data Augmentation** \n",
    "Enhances the training dataset with common phrases. This ensures the model learns important everyday expressions that might be underrepresented in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to add common word pairs to the dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "def add_common_words_to_dataset(ds_raw):\n",
    "    \"\"\"Add common word pairs to ensure they're properly translated\"\"\"\n",
    "    try:\n",
    "        # Get original items as a list\n",
    "        original_items = ds_raw.to_list()\n",
    "\n",
    "        # Create dataset with common words and phrases\n",
    "        common_phrases = [\n",
    "            {\"translation\": {\"en\": \"Hello\", \"it\": \"Ciao\"}},\n",
    "            {\"translation\": {\"en\": \"Hello, how are you?\", \"it\": \"Ciao, come stai?\"}},\n",
    "            {\"translation\": {\"en\": \"Goodbye\", \"it\": \"Arrivederci\"}},\n",
    "            {\"translation\": {\"en\": \"Thank you\", \"it\": \"Grazie\"}},\n",
    "            {\"translation\": {\"en\": \"Please\", \"it\": \"Per favore\"}},\n",
    "            {\"translation\": {\"en\": \"Yes\", \"it\": \"SÃ¬\"}},\n",
    "            {\"translation\": {\"en\": \"No\", \"it\": \"No\"}},\n",
    "            {\"translation\": {\"en\": \"Good morning\", \"it\": \"Buongiorno\"}},\n",
    "            {\"translation\": {\"en\": \"Good evening\", \"it\": \"Buonasera\"}},\n",
    "            {\"translation\": {\"en\": \"Good night\", \"it\": \"Buonanotte\"}},\n",
    "            {\"translation\": {\"en\": \"How are you?\", \"it\": \"Come stai?\"}},\n",
    "            {\"translation\": {\"en\": \"My name is\", \"it\": \"Mi chiamo\"}},\n",
    "            {\"translation\": {\"en\": \"What is your name?\", \"it\": \"Come ti chiami?\"}},\n",
    "            {\"translation\": {\"en\": \"I don't understand\", \"it\": \"Non capisco\"}},\n",
    "            {\"translation\": {\"en\": \"I love you\", \"it\": \"Ti amo\"}},\n",
    "            {\"translation\": {\"en\": \"I'm sorry\", \"it\": \"Mi dispiace\"}},\n",
    "            {\"translation\": {\"en\": \"Where is\", \"it\": \"Dov'Ã¨\"}},\n",
    "            {\"translation\": {\"en\": \"How much is this?\", \"it\": \"Quanto costa?\"}},\n",
    "            {\"translation\": {\"en\": \"I would like\", \"it\": \"Vorrei\"}},\n",
    "            {\"translation\": {\"en\": \"Can you help me?\", \"it\": \"Puoi aiutarmi?\"}},\n",
    "        ]\n",
    "\n",
    "        # Add 5 copies of common phrases for emphasis\n",
    "        enhanced_items = original_items.copy()\n",
    "        for _ in range(5):\n",
    "            enhanced_items.extend(common_phrases)\n",
    "\n",
    "        # Create new dataset\n",
    "        enhanced_ds = Dataset.from_list(enhanced_items)\n",
    "\n",
    "        print(f\"Original dataset size: {len(ds_raw)}\")\n",
    "        print(f\"Enhanced dataset size: {len(enhanced_ds)}\")\n",
    "\n",
    "        return enhanced_ds\n",
    "    except Exception as e:\n",
    "        print(f\"Error augmenting dataset: {str(e)}\")\n",
    "        print(\"Using original dataset instead\")\n",
    "        return ds_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¤ **Tokenizer Construction** \n",
    "Creates improved tokenizers that preserve all vocabulary items. The `min_frequency=1` setting ensures even rare words are included in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_sentences(ds, lang):\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]\n",
    "\n",
    "def get_or_build_improved_tokenizer(config, ds, lang):\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        # Most code taken from: https://huggingface.co/docs/tokenizers/quicktour\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=1)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”„ **Dataset Preparation**\n",
    "Prepares and splits the dataset with augmentations. This function handles loading, preprocessing, and creating efficient data loaders for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create function to get datasets with our improvements\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from datasets import load_dataset\n",
    "from dataset import BilingualDataset\n",
    "\n",
    "def get_improved_ds(config):\n",
    "    \"\"\"Get datasets with data augmentation and improved tokenization\"\"\"\n",
    "    # Load dataset\n",
    "    print(\"Loading dataset...\")\n",
    "    ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n",
    "\n",
    "    # Apply data augmentation for common words\n",
    "    print(\"Enhancing dataset with common words...\")\n",
    "    enhanced_ds = add_common_words_to_dataset(ds_raw)\n",
    "\n",
    "    # Build improved tokenizers\n",
    "    print(\"Building tokenizers...\")\n",
    "    tokenizer_src = get_or_build_improved_tokenizer(config, enhanced_ds, config['lang_src'])\n",
    "    tokenizer_tgt = get_or_build_improved_tokenizer(config, enhanced_ds, config['lang_tgt'])\n",
    "\n",
    "    # Split dataset\n",
    "    print(\"Splitting dataset...\")\n",
    "    train_ds_size = int(0.9 * len(enhanced_ds))\n",
    "    val_ds_size = len(enhanced_ds) - train_ds_size\n",
    "    train_raw, val_raw = random_split(enhanced_ds, [train_ds_size, val_ds_size])\n",
    "\n",
    "    # Create bilingual datasets\n",
    "    print(\"Creating datasets...\")\n",
    "    train_ds = BilingualDataset(\n",
    "        ds=train_raw,\n",
    "        tokenizer_src=tokenizer_src,\n",
    "        tokenizer_tgt=tokenizer_tgt,\n",
    "        src_lang=config['lang_src'],\n",
    "        tgt_lang=config['lang_tgt'],\n",
    "        seq_len=config['seq_len']\n",
    "    )\n",
    "\n",
    "    val_ds = BilingualDataset(\n",
    "        ds=val_raw,\n",
    "        tokenizer_src=tokenizer_src,\n",
    "        tokenizer_tgt=tokenizer_tgt,\n",
    "        src_lang=config['lang_src'],\n",
    "        tgt_lang=config['lang_tgt'],\n",
    "        seq_len=config['seq_len']\n",
    "    )\n",
    "\n",
    "    # Create data loaders - using train_ds and val_ds consistently\n",
    "    train_dataloader = DataLoader(\n",
    "        train_ds,  # Changed from train_dataset to train_ds\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_dataloader = DataLoader(\n",
    "        val_ds,  # Changed from val_dataset to val_ds\n",
    "        batch_size=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install torchmetrics directly in Colab\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ‹ï¸ **Training Function**\n",
    "Implements an advanced training loop with learning rate scheduling and validation metrics. The function includes best model saving based on BLEU score performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create improved training function with learning rate scheduler and label smoothing\n",
    "from model import build_transformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchmetrics  # Now this should work\n",
    "from config import get_weights_file_path, latest_weights_file_path\n",
    "\n",
    "def train_improved_model(config):\n",
    "    \"\"\"Improved training function with optimizations\"\"\"\n",
    "\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    if device == 'cuda':\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Device memory: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3} GB\")\n",
    "\n",
    "    # Create weights directory\n",
    "    weights_path = Path(f\"{config['datasource']}_{config['model_folder']}\")\n",
    "    weights_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Get datasets\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_improved_ds(config)\n",
    "\n",
    "    # Build model\n",
    "    model = build_transformer(\n",
    "        tokenizer_src.get_vocab_size(),\n",
    "        tokenizer_tgt.get_vocab_size(),\n",
    "        config['seq_len'],\n",
    "        config['seq_len'],\n",
    "        d_model=config['d_model']\n",
    "    ).to(device)\n",
    "\n",
    "    # Initialize TensorBoard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    # Optimizer with better parameters\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=config['lr'],\n",
    "        betas=(0.9, 0.98),\n",
    "        eps=1e-9\n",
    "    )\n",
    "\n",
    "    # Learning rate scheduler for better convergence\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=config['lr'],\n",
    "        steps_per_epoch=len(train_dataloader),\n",
    "        epochs=config['num_epochs'],\n",
    "        pct_start=0.1,\n",
    "        div_factor=10,\n",
    "        final_div_factor=100\n",
    "    )\n",
    "\n",
    "    # Check for preloaded model\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "    preload = config['preload']\n",
    "    model_filename = latest_weights_file_path(config) if preload == 'latest' else get_weights_file_path(config, preload) if preload else None\n",
    "\n",
    "    if model_filename and Path(model_filename).exists():\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename, map_location=device)\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        global_step = state['global_step']\n",
    "    else:\n",
    "        print('No model to preload, starting from scratch')\n",
    "\n",
    "    # Loss function with label smoothing for better generalization\n",
    "    loss_fn = nn.CrossEntropyLoss(\n",
    "        ignore_index=tokenizer_tgt.token_to_id('[PAD]'),\n",
    "        label_smoothing=0.1\n",
    "    ).to(device)\n",
    "\n",
    "    # Track best validation score\n",
    "    best_bleu = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "\n",
    "        # Training phase\n",
    "        for batch in batch_iterator:\n",
    "            # Get batch data\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            decoder_input = batch['decoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "            decoder_mask = batch['decoder_mask'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "            proj_output = model.project(decoder_output)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(\n",
    "                proj_output.view(-1, tokenizer_tgt.get_vocab_size()),\n",
    "                label.view(-1)\n",
    "            )\n",
    "\n",
    "            # Update progress\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            # Log metrics\n",
    "            writer.add_scalar('train/loss', loss.item(), global_step)\n",
    "            writer.add_scalar('train/learning_rate', scheduler.get_last_lr()[0], global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            # Update learning rate\n",
    "            scheduler.step()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # Validation phase\n",
    "        print(f\"\\nValidation after epoch {epoch+1}:\")\n",
    "        model.eval()\n",
    "\n",
    "        # Collect validation examples\n",
    "        sources = []\n",
    "        targets = []\n",
    "        predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Only process first 100 examples\n",
    "            for count, batch in enumerate(tqdm(val_dataloader, desc=\"Validation\", total=100)):\n",
    "                # Hard limit to 100 examples\n",
    "                if count >= 10:\n",
    "                    break\n",
    "\n",
    "                # Get batch data\n",
    "                encoder_input = batch[\"encoder_input\"].to(device)\n",
    "                encoder_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "                # Generate translation with beam search\n",
    "                model_out = beam_search_decode(\n",
    "                    model, encoder_input, encoder_mask,\n",
    "                    tokenizer_src, tokenizer_tgt,\n",
    "                    config['seq_len'], device\n",
    "                )\n",
    "\n",
    "                # Get text\n",
    "                source_text = batch[\"src_text\"][0]\n",
    "                target_text = batch[\"tgt_text\"][0]\n",
    "                model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n",
    "                model_out_text = model_out_text.replace(\"[SOS]\", \"\").replace(\"[EOS]\", \"\").strip()\n",
    "\n",
    "                # Store for metrics\n",
    "                sources.append(source_text)\n",
    "                targets.append(target_text)\n",
    "                predictions.append(model_out_text)\n",
    "\n",
    "                # Print examples\n",
    "                if count < 3:\n",
    "                    print(f\"Example {count+1}:\")\n",
    "                    print(f\"Source: {source_text}\")\n",
    "                    print(f\"Target: {target_text}\")\n",
    "                    print(f\"Predicted: {model_out_text}\")\n",
    "                    print(\"-\" * 80)\n",
    "\n",
    "        # Calculate metrics\n",
    "        metric = torchmetrics.BLEUScore()\n",
    "        bleu_score = metric(predictions, [[t] for t in targets])\n",
    "\n",
    "        metric = torchmetrics.WordErrorRate()\n",
    "        wer = metric(predictions, targets)\n",
    "\n",
    "        # Log metrics\n",
    "        writer.add_scalar('validation/BLEU', bleu_score, global_step)\n",
    "        writer.add_scalar('validation/WER', wer, global_step)\n",
    "        writer.flush()\n",
    "\n",
    "        print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "        print(f\"Word Error Rate: {wer:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if bleu_score > best_bleu:\n",
    "            best_bleu = bleu_score\n",
    "            best_model_path = get_weights_file_path(config, \"best\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'global_step': global_step,\n",
    "                'bleu_score': bleu_score\n",
    "            }, best_model_path)\n",
    "            print(f\"New best model (BLEU: {bleu_score:.4f}) saved to {best_model_path}\")\n",
    "\n",
    "        # Save epoch checkpoint\n",
    "        model_filename = get_weights_file_path(config, f\"{epoch:02d}\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'global_step': global_step,\n",
    "            'bleu_score': bleu_score\n",
    "        }, model_filename)\n",
    "        print(f\"Saved checkpoint to {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸš€ **Execute Training** \n",
    "Runs the complete training process for the specified number of epochs. This is the main execution cell that trains the model using all previous setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Run the improved training\n",
    "train_improved_model(config)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
