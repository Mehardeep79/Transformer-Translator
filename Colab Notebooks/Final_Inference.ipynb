{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPYa4dtW+RBnr5uL1ySttHJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **üìò Project Title: Transformer-Based Text Translation**\n","A practical implementation of a Transformer model for language translation.\n","\n","# üß† **Overview**\n","This notebook demonstrates the inference pipeline of a trained Transformer model for text translation. It showcases the complete utilization of the trained model on the Opus Books \"en-it\" dataset to perform translation on sample sentences as well as custom sentences."],"metadata":{"id":"54pVFddWFknR"}},{"cell_type":"markdown","source":["# üõ†Ô∏è  **Environment** **Setup**\n","Set Up Virtual Environment and Install Dependencies"],"metadata":{"id":"h59F8K4aGt2_"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pkU2cy3np4c5","executionInfo":{"status":"ok","timestamp":1745395465727,"user_tz":-330,"elapsed":52,"user":{"displayName":"Mehardeep Sandhu","userId":"04190069470072931242"}},"outputId":"29cb0608-5f64-44db-d7eb-23d2fdb951e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["env: PYTHONPATH=\n"]}],"source":["%env PYTHONPATH ="]},{"cell_type":"code","source":["!pip install virtualenv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"10npxLmnqM7p","executionInfo":{"status":"ok","timestamp":1745395486744,"user_tz":-330,"elapsed":18079,"user":{"displayName":"Mehardeep Sandhu","userId":"04190069470072931242"}},"outputId":"a1141e98-fa77-41ae-e72b-f0b66f18adf2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting virtualenv\n","  Downloading virtualenv-20.30.0-py3-none-any.whl.metadata (4.5 kB)\n","Collecting distlib<1,>=0.3.7 (from virtualenv)\n","  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n","Requirement already satisfied: filelock<4,>=3.12.2 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (3.18.0)\n","Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv) (4.3.7)\n","Downloading virtualenv-20.30.0-py3-none-any.whl (4.3 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: distlib, virtualenv\n","Successfully installed distlib-0.3.9 virtualenv-20.30.0\n"]}]},{"cell_type":"code","source":["!virtualenv myenv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QOs0z1eiqRWd","executionInfo":{"status":"ok","timestamp":1745395517625,"user_tz":-330,"elapsed":18265,"user":{"displayName":"Mehardeep Sandhu","userId":"04190069470072931242"}},"outputId":"259834e0-9994-45ab-a02e-ce069b1dbe16"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["created virtual environment CPython3.11.12.final.0-64 in 2144ms\n","  creator CPython3Posix(dest=/content/myenv, clear=False, no_vcs_ignore=False, global=False)\n","  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/root/.local/share/virtualenv)\n","    added seed packages: pip==25.0.1, setuptools==78.1.0, wheel==0.45.1\n","  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"]}]},{"cell_type":"code","source":["!myenv/bin/python --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vBfhwMlqqVAq","executionInfo":{"status":"ok","timestamp":1745395521598,"user_tz":-330,"elapsed":106,"user":{"displayName":"Mehardeep Sandhu","userId":"04190069470072931242"}},"outputId":"5e759ffb-0498-43c8-bfa2-1d049d16389f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.11.12\n"]}]},{"cell_type":"markdown","source":["Installing all the necessary dependecies with their correct versions as given in the `requirements.txt`:\n"],"metadata":{"id":"gN5wWS4JHzED"}},{"cell_type":"code","source":["# Make sure we're using the virtual environment's pip\n","!myenv/bin/pip install numpy==1.24.3\n","!myenv/bin/pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2\n","!myenv/bin/pip install datasets==2.15.0 tokenizers==0.13.3 torchmetrics==1.0.3\n","!myenv/bin/pip install tensorboard==2.13.0 tqdmn altair==5.1.1 wandb==0.15.9"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"01oo1zwaqYJy","executionInfo":{"status":"ok","timestamp":1745395696328,"user_tz":-330,"elapsed":172140,"user":{"displayName":"Mehardeep Sandhu","userId":"04190069470072931242"}},"outputId":"754c4ae0-ff0c-4034-cacf-7324ea792c9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting numpy==1.24.3\n","  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n","Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m84.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","Successfully installed numpy-1.24.3\n","Collecting torch==2.0.1\n","  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n","Collecting torchvision==0.15.2\n","  Downloading torchvision-0.15.2-cp311-cp311-manylinux1_x86_64.whl.metadata (11 kB)\n","Collecting torchaudio==2.0.2\n","  Downloading torchaudio-2.0.2-cp311-cp311-manylinux1_x86_64.whl.metadata (1.2 kB)\n","Collecting filelock (from torch==2.0.1)\n","  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n","Collecting typing-extensions (from torch==2.0.1)\n","  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n","Collecting sympy (from torch==2.0.1)\n","  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n","Collecting networkx (from torch==2.0.1)\n","  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n","Collecting jinja2 (from torch==2.0.1)\n","  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n","Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n","  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n","  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n","  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n","  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n","  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n","  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n","  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Collecting triton==2.0.0 (from torch==2.0.1)\n","  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n","Requirement already satisfied: numpy in ./myenv/lib/python3.11/site-packages (from torchvision==0.15.2) (1.24.3)\n","Collecting requests (from torchvision==0.15.2)\n","  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n","Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.15.2)\n","  Downloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n","Requirement already satisfied: setuptools in ./myenv/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (78.1.0)\n","Requirement already satisfied: wheel in ./myenv/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\n","Collecting cmake (from triton==2.0.0->torch==2.0.1)\n","  Downloading cmake-4.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n","Collecting lit (from triton==2.0.0->torch==2.0.1)\n","  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n","Collecting MarkupSafe>=2.0 (from jinja2->torch==2.0.1)\n","  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n","Collecting charset-normalizer<4,>=2 (from requests->torchvision==0.15.2)\n","  Downloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n","Collecting idna<4,>=2.5 (from requests->torchvision==0.15.2)\n","  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n","Collecting urllib3<3,>=1.21.1 (from requests->torchvision==0.15.2)\n","  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n","Collecting certifi>=2017.4.17 (from requests->torchvision==0.15.2)\n","  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n","Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.0.1)\n","  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n","Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchvision-0.15.2-cp311-cp311-manylinux1_x86_64.whl (6.0 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m104.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchaudio-2.0.2-cp311-cp311-manylinux1_x86_64.whl (4.4 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m117.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n","Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pillow-11.2.1-cp311-cp311-manylinux_2_28_x86_64.whl (4.6 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n","Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n","Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n","Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n","Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n","Downloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n","Downloading idna-3.10-py3-none-any.whl (70 kB)\n","Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n","Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n","Downloading cmake-4.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.9 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n","Installing collected packages: mpmath, lit, urllib3, typing-extensions, sympy, pillow, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, networkx, MarkupSafe, idna, filelock, cmake, charset-normalizer, certifi, requests, nvidia-cusolver-cu11, nvidia-cudnn-cu11, jinja2, triton, torch, torchvision, torchaudio\n","Successfully installed MarkupSafe-3.0.2 certifi-2025.1.31 charset-normalizer-3.4.1 cmake-4.0.0 filelock-3.18.0 idna-3.10 jinja2-3.1.6 lit-18.1.8 mpmath-1.3.0 networkx-3.4.2 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 pillow-11.2.1 requests-2.32.3 sympy-1.13.3 torch-2.0.1 torchaudio-2.0.2 torchvision-0.15.2 triton-2.0.0 typing-extensions-4.13.2 urllib3-2.4.0\n","Collecting datasets==2.15.0\n","  Downloading datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n","Collecting tokenizers==0.13.3\n","  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Collecting torchmetrics==1.0.3\n","  Downloading torchmetrics-1.0.3-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: numpy>=1.17 in ./myenv/lib/python3.11/site-packages (from datasets==2.15.0) (1.24.3)\n","Collecting pyarrow>=8.0.0 (from datasets==2.15.0)\n","  Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n","Collecting pyarrow-hotfix (from datasets==2.15.0)\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets==2.15.0)\n","  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n","Collecting pandas (from datasets==2.15.0)\n","  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n","Requirement already satisfied: requests>=2.19.0 in ./myenv/lib/python3.11/site-packages (from datasets==2.15.0) (2.32.3)\n","Collecting tqdm>=4.62.1 (from datasets==2.15.0)\n","  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n","Collecting xxhash (from datasets==2.15.0)\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess (from datasets==2.15.0)\n","  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n","Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.15.0)\n","  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n","Collecting aiohttp (from datasets==2.15.0)\n","  Downloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n","Collecting huggingface-hub>=0.18.0 (from datasets==2.15.0)\n","  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n","Collecting packaging (from datasets==2.15.0)\n","  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n","Collecting pyyaml>=5.1 (from datasets==2.15.0)\n","  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n","Requirement already satisfied: torch>=1.8.1 in ./myenv/lib/python3.11/site-packages (from torchmetrics==1.0.3) (2.0.1)\n","Collecting lightning-utilities>=0.7.0 (from torchmetrics==1.0.3)\n","  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n","Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets==2.15.0)\n","  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n","Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.15.0)\n","  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n","Collecting attrs>=17.3.0 (from aiohttp->datasets==2.15.0)\n","  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n","Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.15.0)\n","  Downloading frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n","Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.15.0)\n","  Downloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n","Collecting propcache>=0.2.0 (from aiohttp->datasets==2.15.0)\n","  Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n","Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==2.15.0)\n","  Downloading yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n","Requirement already satisfied: filelock in ./myenv/lib/python3.11/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (3.18.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in ./myenv/lib/python3.11/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (4.13.2)\n","Requirement already satisfied: setuptools in ./myenv/lib/python3.11/site-packages (from lightning-utilities>=0.7.0->torchmetrics==1.0.3) (78.1.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.15.0) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.15.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.15.0) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.11/site-packages (from requests>=2.19.0->datasets==2.15.0) (2025.1.31)\n","Requirement already satisfied: sympy in ./myenv/lib/python3.11/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (1.13.3)\n","Requirement already satisfied: networkx in ./myenv/lib/python3.11/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (3.4.2)\n","Requirement already satisfied: jinja2 in ./myenv/lib/python3.11/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in ./myenv/lib/python3.11/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in ./myenv/lib/python3.11/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in ./myenv/lib/python3.11/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (11.7.101)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in ./myenv/lib/python3.11/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in ./myenv/lib/python3.11/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (11.10.3.66)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in ./myenv/lib/python3.11/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in ./myenv/lib/python3.11/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (10.2.10.91)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in ./myenv/lib/python3.11/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (11.4.0.1)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in ./myenv/lib/python3.11/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (11.7.4.91)\n","Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in ./myenv/lib/python3.11/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (2.14.3)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in ./myenv/lib/python3.11/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (11.7.91)\n","Requirement already satisfied: triton==2.0.0 in ./myenv/lib/python3.11/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (2.0.0)\n","Requirement already satisfied: wheel in ./myenv/lib/python3.11/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.1->torchmetrics==1.0.3) (0.45.1)\n","Requirement already satisfied: cmake in ./myenv/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics==1.0.3) (4.0.0)\n","Requirement already satisfied: lit in ./myenv/lib/python3.11/site-packages (from triton==2.0.0->torch>=1.8.1->torchmetrics==1.0.3) (18.1.8)\n","INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n","Collecting multiprocess (from datasets==2.15.0)\n","  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","  Downloading multiprocess-0.70.15-py311-none-any.whl.metadata (7.2 kB)\n","Collecting python-dateutil>=2.8.2 (from pandas->datasets==2.15.0)\n","  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n","Collecting pytz>=2020.1 (from pandas->datasets==2.15.0)\n","  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n","Collecting tzdata>=2022.7 (from pandas->datasets==2.15.0)\n","  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->datasets==2.15.0)\n","  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.11/site-packages (from jinja2->torch>=1.8.1->torchmetrics==1.0.3) (3.0.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.11/site-packages (from sympy->torch>=1.8.1->torchmetrics==1.0.3) (1.3.0)\n","Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n","Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m90.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading torchmetrics-1.0.3-py3-none-any.whl (731 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m731.6/731.6 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.7-py3-none-any.whl (115 kB)\n","Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n","Downloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n","Downloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n","Downloading packaging-25.0-py3-none-any.whl (66 kB)\n","Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n","Downloading multiprocess-0.70.15-py311-none-any.whl (135 kB)\n","Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m141.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n","Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n","Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n","Downloading frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n","Downloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n","Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n","Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n","Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n","Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n","Downloading yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n","Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n","Installing collected packages: tokenizers, pytz, xxhash, tzdata, tqdm, six, pyyaml, pyarrow-hotfix, pyarrow, propcache, packaging, multidict, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, python-dateutil, multiprocess, lightning-utilities, huggingface-hub, aiosignal, pandas, aiohttp, datasets, torchmetrics\n","Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 attrs-25.3.0 datasets-2.15.0 dill-0.3.7 frozenlist-1.6.0 fsspec-2023.10.0 huggingface-hub-0.30.2 lightning-utilities-0.14.3 multidict-6.4.3 multiprocess-0.70.15 packaging-25.0 pandas-2.2.3 propcache-0.3.1 pyarrow-19.0.1 pyarrow-hotfix-0.6 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.2 six-1.17.0 tokenizers-0.13.3 torchmetrics-1.0.3 tqdm-4.67.1 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.0\n","Collecting tensorboard==2.13.0\n","  Downloading tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement tqdmn (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for tqdmn\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gxcD0Zv4qdCN","executionInfo":{"status":"ok","timestamp":1745395709432,"user_tz":-330,"elapsed":9389,"user":{"displayName":"Mehardeep Sandhu","userId":"04190069470072931242"}},"outputId":"99f47d30-8bc2-4bf8-c088-791a40fd2ed2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Collecting xxhash (from datasets)\n","  Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","Installing collected packages: xxhash, fsspec, dill, multiprocess, datasets\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2025.3.2\n","    Uninstalling fsspec-2025.3.2:\n","      Successfully uninstalled fsspec-2025.3.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}]},{"cell_type":"markdown","source":["# üóÇÔ∏è **Mount Google Drive**\n","Imports Google Drive interface module for mounting cloud storage. Mounts your Google Drive to Colab, allowing file read/write access.\n","\n","Changes into the working directory to the project folder in Drive."],"metadata":{"id":"D2DGR8iOH21a"}},{"cell_type":"code","source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Get in project directory\n","%cd /content/drive/MyDrive/transformer_project_major"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5TYvf9USqZEl","executionInfo":{"status":"ok","timestamp":1745395743980,"user_tz":-330,"elapsed":22198,"user":{"displayName":"Mehardeep Sandhu","userId":"04190069470072931242"}},"outputId":"4e997fcb-e90f-4aa4-d780-41a88b0bbee2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/transformer_project_major\n"]}]},{"cell_type":"markdown","source":["# üì¶ **Import all the needed libraries**\n","\n","Import `Torch Utils` from DataLoader which Facilitates efficient data loading in batches, shuffling, and parallel processing during training.\n","\n","Imports the `Dataset` class from Hugging Face for creating and managing custom datasets.\n","Used for batching data and splitting the dataset into training and validation sets.\n","\n","Imports the base `Tokenizer` from the Hugging Face tokenizers library. This class handles the encoding and decoding of text to tokens.\n","\n","Imports `tokenizer trainer` which is used to create a word-level vocabulary from the training data, including special tokens like [PAD], [SOS], and [EOS].\n","\n","Imports `pre_tokenizer` from Whitespace library. It splits text into tokens based on whitespace ‚Äî a straightforward way to prepare text before training the tokenizer.\n","\n","Imports all the other important functions from the already defined files like: model.py, dataset.py, config.py"],"metadata":{"id":"cmjvWn4pIGrP"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","from model import Transformer, build_transformer\n","from dataset import BilingualDataset, causal_mask\n","from config import get_config"],"metadata":{"id":"BWtofquwqmS4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","from tokenizers import Tokenizer\n","from tokenizers.models import WordLevel\n","from tokenizers.trainers import WordLevelTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","from pathlib import Path"],"metadata":{"id":"_R1h52DuqqFw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üîç **Creating the Beam Search Function**\n","The `beam_search_decode` function is a decoding algorithm used during inference in machine translation (or similar NLP tasks) with a Transformer model. Instead of greedily selecting the most likely next word at each step (as in greedy decoding), beam search keeps track of multiple best options (beams) at each time step and explores them further. This results in translations that are often more fluent and accurate."],"metadata":{"id":"Bzex2SY4IPWS"}},{"cell_type":"code","source":["# Create an improved beam search function for inference\n","import torch\n","from dataset import causal_mask\n","\n","def beam_search_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device, beam_size=5):\n","    \"\"\"Beam search for better translation quality\"\"\"\n","    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n","    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n","\n","    # Encode the source sentence\n","    encoder_output = model.encode(source, source_mask)\n","\n","    # Initialize the beam with start token\n","    sequences = [(torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device), 0.0)]\n","\n","    # Beam search\n","    for _ in range(max_len):\n","        new_sequences = []\n","\n","        # Expand each current sequence\n","        for seq, score in sequences:\n","            # If sequence ended with EOS, keep it unchanged\n","            if seq.size(1) > 1 and seq[0, -1].item() == eos_idx:\n","                new_sequences.append((seq, score))\n","                continue\n","\n","            # Create decoder mask for this sequence\n","            decoder_mask = causal_mask(seq.size(1)).type_as(source_mask).to(device)\n","\n","            # Get next token probabilities\n","            out = model.decode(encoder_output, source_mask, seq, decoder_mask)\n","            prob = model.project(out[:, -1])\n","            log_prob = torch.log_softmax(prob, dim=-1)\n","\n","            # Get top-k token candidates\n","            topk_probs, topk_indices = torch.topk(log_prob, beam_size, dim=1)\n","\n","            # Add new candidates to the list\n","            for i in range(beam_size):\n","                token = topk_indices[0, i].unsqueeze(0).unsqueeze(0)\n","                new_seq = torch.cat([seq, token], dim=1)\n","                new_score = score + topk_probs[0, i].item()\n","                new_sequences.append((new_seq, new_score))\n","\n","        # Select top-k sequences\n","        new_sequences.sort(key=lambda x: x[1], reverse=True)\n","        sequences = new_sequences[:beam_size]\n","\n","        # Check if all sequences have ended or reached max length\n","        if all((seq.size(1) > 1 and seq[0, -1].item() == eos_idx) or seq.size(1) >= max_len\n","               for seq, _ in sequences):\n","            break\n","\n","    # Return the best sequence\n","    return sequences[0][0].squeeze(0)"],"metadata":{"id":"lT3yOylsqyrD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This shall load the best model which here we are considering the `tmodel30.pt` i.e is the 30th epoch trained model and its BLEU score for the translations."],"metadata":{"id":"2CDQToiYId9V"}},{"cell_type":"code","source":["# Load the 30th epoch model for inference\n","from model import build_transformer\n","import torch\n","from config import get_config, get_weights_file_path\n","from tokenizers import Tokenizer\n","from pathlib import Path\n","\n","# Get configuration\n","cfg = get_config()\n","cfg['model_folder'] = 'weights'\n","cfg['tokenizer_file'] = 'vocab/tokenizer_{0}.json'\n","\n","# Load tokenizers\n","tokenizer_src = Tokenizer.from_file(cfg['tokenizer_file'].format(cfg['lang_src']))\n","tokenizer_tgt = Tokenizer.from_file(cfg['tokenizer_file'].format(cfg['lang_tgt']))\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","# Build model\n","model = build_transformer(\n","    tokenizer_src.get_vocab_size(),\n","    tokenizer_tgt.get_vocab_size(),\n","    cfg['seq_len'],\n","    cfg['seq_len'],\n","    d_model=cfg['d_model']\n",").to(device)\n","\n","# Directly load the 30th epoch model\n","model_path = get_weights_file_path(cfg, \"30\")\n","\n","# Check if the file exists\n","if Path(model_path).exists():\n","    state = torch.load(model_path, map_location=device)\n","    model.load_state_dict(state['model_state_dict'])\n","    model.eval()\n","    print(f\"Loaded 30th epoch model from {model_path}\")\n","    print(f\"BLEU score: {state.get('bleu_score', 'N/A')}\")\n","else:\n","    print(f\"30th epoch model not found at {model_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RxyFffAIq1-a","executionInfo":{"status":"ok","timestamp":1745396129196,"user_tz":-330,"elapsed":26759,"user":{"displayName":"Mehardeep Sandhu","userId":"04190069470072931242"}},"outputId":"ac6988cc-b713-4715-ed49-758940d8c4d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cpu\n","Loaded 30th epoch model from opus_books_weights/tmodel_30.pt\n","BLEU score: 0.28094117647058825\n"]}]},{"cell_type":"markdown","source":["# üó£Ô∏è **Translation Function**\n","Creates a utility function for translating text using the trained model. This function handles tokenization, beam search decoding, and post-processing."],"metadata":{"id":"f-ymN4XuJI3H"}},{"cell_type":"code","source":["# Define translation function with beam search\n","def translate(sentence, model, tokenizer_src, tokenizer_tgt, max_len, device, beam_size=5):\n","    \"\"\"Translate a sentence using beam search\"\"\"\n","    model.eval()\n","\n","    # Tokenize the source sentence\n","    tokens = tokenizer_src.encode(sentence).ids\n","\n","    # Add SOS and EOS tokens\n","    tokens = [tokenizer_src.token_to_id('[SOS]')] + tokens + [tokenizer_src.token_to_id('[EOS]')]\n","\n","    # Convert to tensor and create mask\n","    src = torch.LongTensor([tokens]).to(device)\n","    src_mask = (src != tokenizer_src.token_to_id('[PAD]')).unsqueeze(1).unsqueeze(1).int().to(device)\n","\n","    # Translate with beam search\n","    output_tokens = beam_search_decode(\n","        model, src, src_mask, tokenizer_src, tokenizer_tgt, max_len, device, beam_size\n","    )\n","\n","    # Convert tokens to text\n","    output_text = tokenizer_tgt.decode(output_tokens.detach().cpu().numpy())\n","\n","    # Remove special tokens\n","    output_text = output_text.replace('[SOS]', '').replace('[EOS]', '').strip()\n","\n","    return output_text"],"metadata":{"id":"LStUpYFQrGfh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# üß† **Model Inference**\n","Loads the best trained model i.e the 30th epoch model in our case as its BLEU score was the highest, and tests it on example sentences. This demonstrates how well the model translates a variety of common phrases."],"metadata":{"id":"Fh-hxmHxJhNS"}},{"cell_type":"code","source":["# Test with example sentences\n","test_sentences = [\n","    \"Hello, how are you?\",\n","    \"I like to read books.\",\n","    \"What is your name?\",\n","    \"The weather is nice today.\",\n","    \"Thank you for your help.\",\n","    \"Goodbye, see you tomorrow.\",\n","    \"Can you help me?\",\n","    \"I don't understand.\",\n","    \"Please speak more slowly.\",\n","    \"Where is the bathroom?\"\n","]\n","\n","print(\"\\nTesting with example sentences:\")\n","print(\"-\" * 80)\n","\n","for sentence in test_sentences:\n","    translation = translate(sentence, model, tokenizer_src, tokenizer_tgt, cfg['seq_len'], device)\n","    print(f\"EN: {sentence}\")\n","    print(f\"IT: {translation}\")\n","    print(\"-\" * 80)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_SdntDZPrJYl","executionInfo":{"status":"ok","timestamp":1745396166152,"user_tz":-330,"elapsed":20374,"user":{"displayName":"Mehardeep Sandhu","userId":"04190069470072931242"}},"outputId":"6653174e-271f-421d-8209-320b36b14862"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Testing with example sentences:\n","--------------------------------------------------------------------------------\n","EN: Hello, how are you?\n","IT: Ciao , come stai ?\n","--------------------------------------------------------------------------------\n","EN: I like to read books.\n","IT: Mi piace leggere i libri .\n","--------------------------------------------------------------------------------\n","EN: What is your name?\n","IT: Che cosa volete dire ?\n","--------------------------------------------------------------------------------\n","EN: The weather is nice today.\n","IT: Il tempo √® male .\n","--------------------------------------------------------------------------------\n","EN: Thank you for your help.\n","IT: Grazie a te , per il tuo aiuto .\n","--------------------------------------------------------------------------------\n","EN: Goodbye, see you tomorrow.\n","IT: Andiamo , ti prego .\n","--------------------------------------------------------------------------------\n","EN: Can you help me?\n","IT: Forse non vi ?\n","--------------------------------------------------------------------------------\n","EN: I don't understand.\n","IT: Non capisco .\n","--------------------------------------------------------------------------------\n","EN: Please speak more slowly.\n","IT: Per favore , per favore .\n","--------------------------------------------------------------------------------\n","EN: Where is the bathroom?\n","IT: Dov ' √® la scatola ?\n","--------------------------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["# üí¨ **Interactive Interface**\n","Creates a user-friendly interface for real-time translation.\n","\n","This allows testing the model with custom input sentences for practical use.\n"],"metadata":{"id":"Qjy8Lp02J4Uo"}},{"cell_type":"code","source":["# Create interactive translation interface\n","def interactive_translation():\n","    \"\"\"Interactive translation interface\"\"\"\n","    print(\"\\n\" + \"=\" * 80)\n","    print(\"Interactive English to Italian Translator\")\n","    print(\"Enter text to translate (or 'q' to quit)\")\n","    print(\"=\" * 80)\n","\n","    while True:\n","        # Get input from user\n","        sentence = input(\"\\nEN > \")\n","\n","        # Exit if requested\n","        if sentence.lower() == 'q':\n","            break\n","\n","        # Translate\n","        translation = translate(sentence, model, tokenizer_src, tokenizer_tgt, cfg['seq_len'], device)\n","\n","        # Show result\n","        print(f\"IT > {translation}\")\n","\n","# Run the interactive translator\n","interactive_translation()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_95q10rxrL79","executionInfo":{"status":"ok","timestamp":1745396192217,"user_tz":-330,"elapsed":19235,"user":{"displayName":"Mehardeep Sandhu","userId":"04190069470072931242"}},"outputId":"553fd873-36d8-4f1c-f9c8-142d895a6149"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["\n","================================================================================\n","Interactive English to Italian Translator\n","Enter text to translate (or 'q' to quit)\n","================================================================================\n","\n","EN > hello\n","IT > Ciao\n","\n","EN > q\n"]}]}]}