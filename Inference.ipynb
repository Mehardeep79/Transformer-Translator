{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üìò Project Title: Transformer-Based Text Translation**\n",
    "A practical implementation of a Transformer model for language translation.\n",
    "\n",
    "# üß† **Overview**\n",
    "This notebook demonstrates the inference pipeline of a trained Transformer model for text translation. It showcases the complete utilization of the trained model on the Opus Books \"en-it\" dataset to perform translation on sample sentences as well as custom sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è  **Environment** **Setup**\n",
    "Set Up Virtual Environment and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH=\n"
     ]
    }
   ],
   "source": [
    "%env PYTHONPATH ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting virtualenv\n",
      "  Downloading virtualenv-20.30.0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting distlib<1,>=0.3.7 (from virtualenv)\n",
      "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: filelock<4,>=3.12.2 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from virtualenv) (3.17.0)\n",
      "Requirement already satisfied: platformdirs<5,>=3.9.1 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from virtualenv) (4.3.7)\n",
      "Downloading virtualenv-20.30.0-py3-none-any.whl (4.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
      "Installing collected packages: distlib, virtualenv\n",
      "Successfully installed distlib-0.3.9 virtualenv-20.30.0\n"
     ]
    }
   ],
   "source": [
    "!pip install virtualenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created virtual environment CPython3.9.6.final.0-64 in 501ms\n",
      "  creator CPython3macOsFramework(dest=/Users/mehardeepsinghauradine/Desktop/Final_Project/myenv, clear=False, no_vcs_ignore=False, global=False)\n",
      "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=/Users/mehardeepsinghauradine/Library/Application Support/virtualenv)\n",
      "    added seed packages: pip==25.0.1, setuptools==78.1.0, wheel==0.45.1\n",
      "  activators BashActivator,CShellActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
     ]
    }
   ],
   "source": [
    "!virtualenv myenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.6\n"
     ]
    }
   ],
   "source": [
    "!myenv/bin/python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing all the necessary dependecies with their correct versions as given in the `requirements.txt`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.24.3\n",
      "  Downloading numpy-1.24.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Downloading numpy-1.24.3-cp39-cp39-macosx_11_0_arm64.whl (13.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "Successfully installed numpy-1.24.3\n",
      "Collecting torch==2.0.1\n",
      "  Using cached torch-2.0.1-cp39-none-macosx_11_0_arm64.whl.metadata (23 kB)\n",
      "Collecting torchvision==0.15.2\n",
      "  Using cached torchvision-0.15.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting torchaudio==2.0.2\n",
      "  Using cached torchaudio-2.0.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (1.2 kB)\n",
      "Collecting filelock (from torch==2.0.1)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions (from torch==2.0.1)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch==2.0.1)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch==2.0.1)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch==2.0.1)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy in ./myenv/lib/python3.9/site-packages (from torchvision==0.15.2) (1.24.3)\n",
      "Collecting requests (from torchvision==0.15.2)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.15.2)\n",
      "  Downloading pillow-11.2.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (8.9 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch==2.0.1)\n",
      "  Using cached MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->torchvision==0.15.2)\n",
      "  Using cached charset_normalizer-3.4.1-cp39-cp39-macosx_10_9_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->torchvision==0.15.2)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->torchvision==0.15.2)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->torchvision==0.15.2)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.0.1)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached torch-2.0.1-cp39-none-macosx_11_0_arm64.whl (55.8 MB)\n",
      "Using cached torchvision-0.15.2-cp39-cp39-macosx_11_0_arm64.whl (1.4 MB)\n",
      "Using cached torchaudio-2.0.2-cp39-cp39-macosx_11_0_arm64.whl (3.6 MB)\n",
      "Downloading pillow-11.2.1-cp39-cp39-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp39-cp39-macosx_10_9_universal2.whl (197 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: mpmath, urllib3, typing-extensions, sympy, pillow, networkx, MarkupSafe, idna, filelock, charset-normalizer, certifi, requests, jinja2, torch, torchvision, torchaudio\n",
      "Successfully installed MarkupSafe-3.0.2 certifi-2025.1.31 charset-normalizer-3.4.1 filelock-3.18.0 idna-3.10 jinja2-3.1.6 mpmath-1.3.0 networkx-3.2.1 pillow-11.2.1 requests-2.32.3 sympy-1.13.3 torch-2.0.1 torchaudio-2.0.2 torchvision-0.15.2 typing-extensions-4.13.2 urllib3-2.4.0\n",
      "Collecting datasets==2.15.0\n",
      "  Using cached datasets-2.15.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting tokenizers==0.13.3\n",
      "  Using cached tokenizers-0.13.3-cp39-cp39-macosx_12_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting torchmetrics==1.0.3\n",
      "  Using cached torchmetrics-1.0.3-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./myenv/lib/python3.9/site-packages (from datasets==2.15.0) (1.24.3)\n",
      "Collecting pyarrow>=8.0.0 (from datasets==2.15.0)\n",
      "  Using cached pyarrow-19.0.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting pyarrow-hotfix (from datasets==2.15.0)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.15.0)\n",
      "  Using cached dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting pandas (from datasets==2.15.0)\n",
      "  Using cached pandas-2.2.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./myenv/lib/python3.9/site-packages (from datasets==2.15.0) (2.32.3)\n",
      "Collecting tqdm>=4.62.1 (from datasets==2.15.0)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets==2.15.0)\n",
      "  Using cached xxhash-3.5.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==2.15.0)\n",
      "  Downloading multiprocess-0.70.18-py39-none-any.whl.metadata (7.5 kB)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.15.0)\n",
      "  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting aiohttp (from datasets==2.15.0)\n",
      "  Downloading aiohttp-3.11.18-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub>=0.18.0 (from datasets==2.15.0)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting packaging (from datasets==2.15.0)\n",
      "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml>=5.1 (from datasets==2.15.0)\n",
      "  Using cached PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: torch>=1.8.1 in ./myenv/lib/python3.9/site-packages (from torchmetrics==1.0.3) (2.0.1)\n",
      "Collecting lightning-utilities>=0.7.0 (from torchmetrics==1.0.3)\n",
      "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets==2.15.0)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets==2.15.0)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets==2.15.0)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets==2.15.0)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets==2.15.0)\n",
      "  Downloading frozenlist-1.6.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.15.0)\n",
      "  Downloading multidict-6.4.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets==2.15.0)\n",
      "  Downloading propcache-0.3.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==2.15.0)\n",
      "  Downloading yarl-1.20.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.9/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./myenv/lib/python3.9/site-packages (from huggingface-hub>=0.18.0->datasets==2.15.0) (4.13.2)\n",
      "Requirement already satisfied: setuptools in ./myenv/lib/python3.9/site-packages (from lightning-utilities>=0.7.0->torchmetrics==1.0.3) (78.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.15.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.15.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./myenv/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.15.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.9/site-packages (from requests>=2.19.0->datasets==2.15.0) (2025.1.31)\n",
      "Requirement already satisfied: sympy in ./myenv/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./myenv/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in ./myenv/lib/python3.9/site-packages (from torch>=1.8.1->torchmetrics==1.0.3) (3.1.6)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.15.0)\n",
      "  Using cached multiprocess-0.70.17-py39-none-any.whl.metadata (7.2 kB)\n",
      "  Using cached multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "  Using cached multiprocess-0.70.15-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas->datasets==2.15.0)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets==2.15.0)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets==2.15.0)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->datasets==2.15.0)\n",
      "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./myenv/lib/python3.9/site-packages (from jinja2->torch>=1.8.1->torchmetrics==1.0.3) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./myenv/lib/python3.9/site-packages (from sympy->torch>=1.8.1->torchmetrics==1.0.3) (1.3.0)\n",
      "Using cached datasets-2.15.0-py3-none-any.whl (521 kB)\n",
      "Using cached tokenizers-0.13.3-cp39-cp39-macosx_12_0_arm64.whl (3.9 MB)\n",
      "Using cached torchmetrics-1.0.3-py3-none-any.whl (731 kB)\n",
      "Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Using cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Downloading aiohttp-3.11.18-cp39-cp39-macosx_11_0_arm64.whl (457 kB)\n",
      "Downloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Downloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
      "Downloading packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached pyarrow-19.0.1-cp39-cp39-macosx_12_0_arm64.whl (30.7 MB)\n",
      "Using cached PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl (172 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached multiprocess-0.70.15-py39-none-any.whl (133 kB)\n",
      "Using cached pandas-2.2.3-cp39-cp39-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.5.0-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.6.0-cp39-cp39-macosx_11_0_arm64.whl (122 kB)\n",
      "Downloading multidict-6.4.3-cp39-cp39-macosx_11_0_arm64.whl (37 kB)\n",
      "Downloading propcache-0.3.1-cp39-cp39-macosx_11_0_arm64.whl (46 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading yarl-1.20.0-cp39-cp39-macosx_11_0_arm64.whl (95 kB)\n",
      "Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: tokenizers, pytz, xxhash, tzdata, tqdm, six, pyyaml, pyarrow-hotfix, pyarrow, propcache, packaging, multidict, fsspec, frozenlist, dill, attrs, async-timeout, aiohappyeyeballs, yarl, python-dateutil, multiprocess, lightning-utilities, huggingface-hub, aiosignal, torchmetrics, pandas, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.3.0 datasets-2.15.0 dill-0.3.7 frozenlist-1.6.0 fsspec-2023.10.0 huggingface-hub-0.30.2 lightning-utilities-0.14.3 multidict-6.4.3 multiprocess-0.70.15 packaging-25.0 pandas-2.2.3 propcache-0.3.1 pyarrow-19.0.1 pyarrow-hotfix-0.6 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.2 six-1.17.0 tokenizers-0.13.3 torchmetrics-1.0.3 tqdm-4.67.1 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.0\n",
      "Collecting tensorboard==2.13.0\n",
      "  Using cached tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tqdmn (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tqdmn\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Make sure we're using the virtual environment's pip\n",
    "!myenv/bin/pip install numpy==1.24.3\n",
    "!myenv/bin/pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2\n",
    "!myenv/bin/pip install datasets==2.15.0 tokenizers==0.13.3 torchmetrics==1.0.3\n",
    "!myenv/bin/pip install tensorboard==2.13.0 tqdmn altair==5.1.1 wandb==0.15.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (2.4.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: filelock in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.18.0->datasets) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from huggingface-hub>=0.18.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì¶ **Import all the needed libraries**\n",
    "\n",
    "Import `Torch Utils` from DataLoader which Facilitates efficient data loading in batches, shuffling, and parallel processing during training.\n",
    "\n",
    "Imports the `Dataset` class from Hugging Face for creating and managing custom datasets.\n",
    "Used for batching data and splitting the dataset into training and validation sets.\n",
    "\n",
    "Imports the base `Tokenizer` from the Hugging Face tokenizers library. This class handles the encoding and decoding of text to tokens.\n",
    "\n",
    "Imports `tokenizer trainer` which is used to create a word-level vocabulary from the training data, including special tokens like [PAD], [SOS], and [EOS].\n",
    "\n",
    "Imports `pre_tokenizer` from Whitespace library. It splits text into tokens based on whitespace ‚Äî a straightforward way to prepare text before training the tokenizer.\n",
    "\n",
    "Imports all the other important functions from the already defined files like: model.py, dataset.py, config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from model import Transformer, build_transformer\n",
    "from dataset import BilingualDataset, causal_mask\n",
    "from config import get_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mehardeepsinghauradine/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç **Creating the Beam Search Function**\n",
    "The `beam_search_decode` function is a decoding algorithm used during inference in machine translation (or similar NLP tasks) with a Transformer model. Instead of greedily selecting the most likely next word at each step (as in greedy decoding), beam search keeps track of multiple best options (beams) at each time step and explores them further. This results in translations that are often more fluent and accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an improved beam search function for inference\n",
    "import torch\n",
    "from dataset import causal_mask\n",
    "\n",
    "def beam_search_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device, beam_size=5):\n",
    "    \"\"\"Beam search for better translation quality\"\"\"\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Encode the source sentence\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "\n",
    "    # Initialize the beam with start token\n",
    "    sequences = [(torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device), 0.0)]\n",
    "\n",
    "    # Beam search\n",
    "    for _ in range(max_len):\n",
    "        new_sequences = []\n",
    "\n",
    "        # Expand each current sequence\n",
    "        for seq, score in sequences:\n",
    "            # If sequence ended with EOS, keep it unchanged\n",
    "            if seq.size(1) > 1 and seq[0, -1].item() == eos_idx:\n",
    "                new_sequences.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            # Create decoder mask for this sequence\n",
    "            decoder_mask = causal_mask(seq.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "            # Get next token probabilities\n",
    "            out = model.decode(encoder_output, source_mask, seq, decoder_mask)\n",
    "            prob = model.project(out[:, -1])\n",
    "            log_prob = torch.log_softmax(prob, dim=-1)\n",
    "\n",
    "            # Get top-k token candidates\n",
    "            topk_probs, topk_indices = torch.topk(log_prob, beam_size, dim=1)\n",
    "\n",
    "            # Add new candidates to the list\n",
    "            for i in range(beam_size):\n",
    "                token = topk_indices[0, i].unsqueeze(0).unsqueeze(0)\n",
    "                new_seq = torch.cat([seq, token], dim=1)\n",
    "                new_score = score + topk_probs[0, i].item()\n",
    "                new_sequences.append((new_seq, new_score))\n",
    "\n",
    "        # Select top-k sequences\n",
    "        new_sequences.sort(key=lambda x: x[1], reverse=True)\n",
    "        sequences = new_sequences[:beam_size]\n",
    "\n",
    "        # Check if all sequences have ended or reached max length\n",
    "        if all((seq.size(1) > 1 and seq[0, -1].item() == eos_idx) or seq.size(1) >= max_len\n",
    "               for seq, _ in sequences):\n",
    "            break\n",
    "\n",
    "    # Return the best sequence\n",
    "    return sequences[0][0].squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shall load the best model which here we are considering the `tmodel30.pt` i.e is the 30th epoch trained model and its BLEU score for the translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loaded 30th epoch model from opus_books_weights/tmodel_30.pt\n",
      "BLEU score: 0.28094117647058825\n"
     ]
    }
   ],
   "source": [
    "# Load the 30th epoch model for inference\n",
    "from model import build_transformer\n",
    "import torch\n",
    "from config import get_config, get_weights_file_path\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "# Get configuration\n",
    "cfg = get_config()\n",
    "cfg['model_folder'] = 'weights'\n",
    "cfg['tokenizer_file'] = 'vocab/tokenizer_{0}.json'\n",
    "\n",
    "# Load tokenizers\n",
    "tokenizer_src = Tokenizer.from_file(cfg['tokenizer_file'].format(cfg['lang_src']))\n",
    "tokenizer_tgt = Tokenizer.from_file(cfg['tokenizer_file'].format(cfg['lang_tgt']))\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Build model\n",
    "model = build_transformer(\n",
    "    tokenizer_src.get_vocab_size(),\n",
    "    tokenizer_tgt.get_vocab_size(),\n",
    "    cfg['seq_len'],\n",
    "    cfg['seq_len'],\n",
    "    d_model=cfg['d_model']\n",
    ").to(device)\n",
    "\n",
    "# Directly load the 30th epoch model\n",
    "model_path = get_weights_file_path(cfg, \"30\")\n",
    "\n",
    "# Check if the file exists\n",
    "if Path(model_path).exists():\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    model.eval()\n",
    "    print(f\"Loaded 30th epoch model from {model_path}\")\n",
    "    print(f\"BLEU score: {state.get('bleu_score', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"30th epoch model not found at {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üó£Ô∏è **Translation Function** \n",
    "Creates a utility function for translating text using the trained model. This function handles tokenization, beam search decoding, and post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define translation function with beam search\n",
    "def translate(sentence, model, tokenizer_src, tokenizer_tgt, max_len, device, beam_size=5):\n",
    "    \"\"\"Translate a sentence using beam search\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize the source sentence\n",
    "    tokens = tokenizer_src.encode(sentence).ids\n",
    "\n",
    "    # Add SOS and EOS tokens\n",
    "    tokens = [tokenizer_src.token_to_id('[SOS]')] + tokens + [tokenizer_src.token_to_id('[EOS]')]\n",
    "\n",
    "    # Convert to tensor and create mask\n",
    "    src = torch.LongTensor([tokens]).to(device)\n",
    "    src_mask = (src != tokenizer_src.token_to_id('[PAD]')).unsqueeze(1).unsqueeze(1).int().to(device)\n",
    "\n",
    "    # Translate with beam search\n",
    "    output_tokens = beam_search_decode(\n",
    "        model, src, src_mask, tokenizer_src, tokenizer_tgt, max_len, device, beam_size\n",
    "    )\n",
    "\n",
    "    # Convert tokens to text\n",
    "    output_text = tokenizer_tgt.decode(output_tokens.detach().cpu().numpy())\n",
    "\n",
    "    # Remove special tokens\n",
    "    output_text = output_text.replace('[SOS]', '').replace('[EOS]', '').strip()\n",
    "\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† **Model Inference** \n",
    "Loads the best trained model i.e the 30th epoch model in our case as its BLEU score was the highest, and tests it on example sentences. This demonstrates how well the model translates a variety of common phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing with example sentences:\n",
      "--------------------------------------------------------------------------------\n",
      "EN: Hello, how are you?\n",
      "IT: Ciao , come stai ?\n",
      "--------------------------------------------------------------------------------\n",
      "EN: I like to read books.\n",
      "IT: Mi piace leggere i libri .\n",
      "--------------------------------------------------------------------------------\n",
      "EN: What is your name?\n",
      "IT: Che cosa volete dire ?\n",
      "--------------------------------------------------------------------------------\n",
      "EN: The weather is nice today.\n",
      "IT: Il tempo √® male .\n",
      "--------------------------------------------------------------------------------\n",
      "EN: Thank you for your help.\n",
      "IT: Grazie a te , per il tuo aiuto .\n",
      "--------------------------------------------------------------------------------\n",
      "EN: Goodbye, see you tomorrow.\n",
      "IT: Andiamo , ti prego .\n",
      "--------------------------------------------------------------------------------\n",
      "EN: Can you help me?\n",
      "IT: Forse non vi ?\n",
      "--------------------------------------------------------------------------------\n",
      "EN: I don't understand.\n",
      "IT: Non capisco .\n",
      "--------------------------------------------------------------------------------\n",
      "EN: Please speak more slowly.\n",
      "IT: Per favore , per favore .\n",
      "--------------------------------------------------------------------------------\n",
      "EN: Where is the bathroom?\n",
      "IT: Dov ' √® la scatola ?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test with example sentences\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I like to read books.\",\n",
    "    \"What is your name?\",\n",
    "    \"The weather is nice today.\",\n",
    "    \"Thank you for your help.\",\n",
    "    \"Goodbye, see you tomorrow.\",\n",
    "    \"Can you help me?\",\n",
    "    \"I don't understand.\",\n",
    "    \"Please speak more slowly.\",\n",
    "    \"Where is the bathroom?\"\n",
    "]\n",
    "\n",
    "print(\"\\nTesting with example sentences:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    translation = translate(sentence, model, tokenizer_src, tokenizer_tgt, cfg['seq_len'], device)\n",
    "    print(f\"EN: {sentence}\")\n",
    "    print(f\"IT: {translation}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí¨ **Interactive Interface** \n",
    "Creates a user-friendly interface for real-time translation. \n",
    "\n",
    "This allows testing the model with custom input sentences for practical use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive translation interface\n",
    "def interactive_translation():\n",
    "    \"\"\"Interactive translation interface\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Interactive English to Italian Translator\")\n",
    "    print(\"Enter text to translate (or 'q' to quit)\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    while True:\n",
    "        # Get input from user\n",
    "        sentence = input(\"\\nEN > \")\n",
    "\n",
    "        # Exit if requested\n",
    "        if sentence.lower() == 'q':\n",
    "            break\n",
    "\n",
    "        # Translate\n",
    "        translation = translate(sentence, model, tokenizer_src, tokenizer_tgt, cfg['seq_len'], device)\n",
    "\n",
    "        # Show result\n",
    "        print(f\"IT > {translation}\")\n",
    "\n",
    "# Run the interactive translator\n",
    "interactive_translation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
